{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOirnF23asdvnNEZeiiI6VM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awaiskhan005/DEEP-LEARNING-AND-FORECASTING/blob/main/Reinforcement_learning_for_Crypto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'shimmy>=2.0'  # Compatibility layer for Gymnasium\n",
        "!pip install gymnasium       # Replace OpenAI Gym with Gymnasium\n",
        "!pip install stable-baselines3  # Reinstall SB3 to ensure compatibility"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qmTZJWcastd",
        "outputId": "855a42d4-3ec3-4740-fdd4-299118a91412"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy>=2.0\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy>=2.0) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NwsPWwqZXhZ",
        "outputId": "95365209-0215-4310-da56-9a6fb022e070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.5.1+cu124)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.1)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.5.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install gym stable-baselines3 pandas numpy yfinance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym  # <-- Use gymnasium instead of gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n"
      ],
      "metadata": {
        "id": "q00LYjzWbo9T"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TradingEnv(gym.Env):\n",
        "    def __init__(self, data, window_size=30):\n",
        "        super(TradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.current_step = self.window_size\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: 6 features (OHLC + Volume + RSI)\n",
        "        self.observation_space = spaces.Box(  # <-- Fix shape here\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 6),  # Now 6 columns!\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    # ... rest of the code ...\n",
        "\n",
        "    def _next_observation(self):\n",
        "        obs = self.data.iloc[self.current_step - self.window_size:self.current_step]\n",
        "        return obs.values\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1:  # Buy\n",
        "            self.position = 1\n",
        "        elif action == 2:  # Sell\n",
        "            self.position = -1\n",
        "        else:  # Hold\n",
        "            pass\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        reward = current_price - prev_price if self.position != 0 else 0\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "\n",
        "        truncated = False  # Gymnasium requires this\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        return self._next_observation(), {}  # Gymnasium expects (obs, info)"
      ],
      "metadata": {
        "id": "oBGfM22nbBDY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch Bitcoin data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))  # <-- Add RSI to DataFrame\n",
        "\n",
        "# Now select the columns (including RSI)\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYkf3fnaaBGg",
        "outputId": "f2a9f255-b354-4baf-a450-369741f09050"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reinitialize and train\n",
        "env = TradingEnv(data)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "XI-nXyv0aI_9",
        "outputId": "7e78a7ec-3558-4df8-c638-ea375b49a928"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected parameter logits (Tensor of shape (1, 3)) of distribution Categorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan]])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6e7bb9a02ee3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MlpPolicy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         )\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m     72\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0;34mf\"({type(value).__name__} of shape {tuple(value.shape)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected parameter logits (Tensor of shape (1, 3)) of distribution Categorical(logits: torch.Size([1, 3])) to satisfy the constraint IndependentConstraint(Real(), 1), but found invalid values:\ntensor([[nan, nan, nan]])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "\n",
        "print(f\"Total simulated profit: {total_reward}\")"
      ],
      "metadata": {
        "id": "Tq6PC2EqbOrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# ======================\n",
        "# 1. Trading Environment\n",
        "# ======================\n",
        "class TradingEnv(gym.Env):\n",
        "    def __init__(self, data, window_size=30):\n",
        "        super(TradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.window_size = window_size\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0  # Current holding position\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: 6 features (OHLC + Volume + RSI)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 6),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Get current window of data\n",
        "        return self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ].values\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        # Update position based on action\n",
        "        if action == 1:  # Buy\n",
        "            self.position = 1\n",
        "        elif action == 2:  # Sell\n",
        "            self.position = -1\n",
        "        # Hold does nothing\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        # Calculate reward (price difference)\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        reward = current_price - prev_price if self.position != 0 else 0\n",
        "\n",
        "        # Check termination\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False  # Gymnasium requirement\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        return self._next_observation(), {}  # (obs, info)\n",
        "\n",
        "# ====================\n",
        "# 2. Data Preparation\n",
        "# ====================\n",
        "# Fetch Bitcoin data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# Select columns and clean NaN values\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# ====================\n",
        "# 3. Train RL Agent\n",
        "# ====================\n",
        "# Create environment\n",
        "env = TradingEnv(data)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"crypto_trading_agent\")\n",
        "\n",
        "# ====================\n",
        "# Reset the vectorized environment\n",
        "obs = env.reset()\n",
        "done = [False]\n",
        "total_reward = 0\n",
        "\n",
        "while not done[0]:\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, rewards, done, _ = env.step(action)\n",
        "    total_reward += rewards[0]\n",
        "\n",
        "print(f\"Total simulated profit: ${total_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHtroVdYcyXQ",
        "outputId": "f42d18d6-dc43-417a-a75a-86be63510c9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/monitor.py:95: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  self.rewards.append(float(reward))\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.05e+03 |\n",
            "|    ep_rew_mean     | 6.33e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 664      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.05e+03    |\n",
            "|    ep_rew_mean          | 6.3e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 611         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008479201 |\n",
            "|    clip_fraction        | 0.0451      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | -8.23e-06   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.41e+06    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00547    |\n",
            "|    value_loss           | 1.67e+07    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.05e+03     |\n",
            "|    ep_rew_mean          | 6.31e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 600          |\n",
            "|    iterations           | 3            |\n",
            "|    time_elapsed         | 10           |\n",
            "|    total_timesteps      | 6144         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0069689024 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 4.11e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.88e+06     |\n",
            "|    n_updates            | 20           |\n",
            "|    policy_gradient_loss | -0.00157     |\n",
            "|    value_loss           | 1.67e+07     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.05e+03     |\n",
            "|    ep_rew_mean          | 6.32e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 556          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 14           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021963757 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.1         |\n",
            "|    explained_variance   | 8.74e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.41e+06     |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.00125     |\n",
            "|    value_loss           | 1.67e+07     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.05e+03     |\n",
            "|    ep_rew_mean          | 6.3e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 552          |\n",
            "|    iterations           | 5            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 10240        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016418587 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.09        |\n",
            "|    explained_variance   | 0.000171     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.93e+06     |\n",
            "|    n_updates            | 40           |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    value_loss           | 1.63e+07     |\n",
            "------------------------------------------\n",
            "Total simulated profit: $6333.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "        self.balance = 0 # initialize balance\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _get_prediction(self, window):\n",
        "        with torch.no_grad():\n",
        "            return self.predictor(torch.FloatTensor(window)).item()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * (1 + self.transaction_cost) # Consider transaction cost on buy\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance += current_price * (1 - self.transaction_cost) # Consider transaction cost on sell\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position  - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data))\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform(prediction.detach().numpy().reshape(1,-1))[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "```python\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "        self.balance = 0 # initialize balance\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _get_prediction(self, window):\n",
        "        with torch.no_grad():\n",
        "            return self.predictor(torch.FloatTensor(window)).item()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * (1 + self.transaction_cost) # Consider transaction cost on buy\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance += current_price * (1 - self.transaction_cost) # Consider transaction cost on sell\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position  - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data))\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform(prediction.detach().numpy().reshape(1,-1))[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "```python\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "        self.balance = 0 # initialize balance\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _get_prediction(self, window):\n",
        "        with torch.no_grad():\n",
        "            return self.predictor(torch.FloatTensor(window)).item()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * (1 + self.transaction_cost) # Consider transaction cost on buy\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance += current_price * (1 - self.transaction_cost) # Consider transaction cost on sell\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position  - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data))\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform(prediction.detach().numpy().reshape(1,-1))[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "```python\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "        self.balance = 0 # initialize balance\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _get_prediction(self, window):\n",
        "        with torch.no_grad():\n",
        "            return self.predictor(torch.FloatTensor(window)).item()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * (1 + self.transaction_cost) # Consider transaction cost on buy\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance += current_price * (1 - self.transaction_cost) # Consider transaction cost on sell\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position  - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data))\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform(prediction.detach().numpy().reshape(1,-1))[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "```python\n",
        "# Import libraries\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# ======================\n",
        "# 1. Data Preparation\n",
        "# ======================\n",
        "# Fetch and preprocess data\n",
        "data = yf.download('BTC-USD', start='2020-01-01', end='2023-01-01')\n",
        "\n",
        "# Calculate RSI\n",
        "delta = data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "data['RSI'] = 100 - (100 / (1 + rs))\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].dropna()\n",
        "\n",
        "# Scale data for LSTM\n",
        "close_scaler = MinMaxScaler()\n",
        "data['Close_scaled'] = close_scaler.fit_transform(data[['Close']])\n",
        "\n",
        "# Create sequences for LSTM\n",
        "sequence_length = 30\n",
        "X, y = [], []\n",
        "for i in range(len(data)-sequence_length-1):\n",
        "    X.append(data['Close_scaled'].values[i:i+sequence_length])\n",
        "    y.append(data['Close_scaled'].values[i+sequence_length+1])  # Predict next day\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split data (80% train, 20% test)\n",
        "split = int(0.8*len(X))\n",
        "X_train, y_train = X[:split], y[:split]\n",
        "X_test, y_test = X[split:], y[split:]\n",
        "\n",
        "# ======================\n",
        "# 2. LSTM Price Predictor\n",
        "# ======================\n",
        "class PricePredictor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(50, 25),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(25, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)  # Add feature dimension\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1])\n",
        "\n",
        "# Initialize model\n",
        "predictor = PricePredictor()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(predictor.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to tensors\n",
        "train_data = torch.utils.data.TensorDataset(torch.FloatTensor(X_train), torch.FloatTensor(y_train))\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# Train LSTM\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        outputs = predictor(batch_x)\n",
        "        loss = criterion(outputs.squeeze(), batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Save predictor\n",
        "torch.save(predictor.state_dict(), \"lstm_predictor.pth\")\n",
        "\n",
        "# ======================\n",
        "# 3. Hybrid RL Environment\n",
        "# ======================\n",
        "class HybridTradingEnv(gym.Env):\n",
        "    def __init__(self, data, predictor):\n",
        "        super().__init__()\n",
        "        self.data = data\n",
        "        self.predictor = predictor\n",
        "        self.window_size = 30\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.transaction_cost = 0.001  # 0.1% per trade\n",
        "        self.balance = 0 # initialize balance\n",
        "\n",
        "        # Action space: 0=hold, 1=buy, 2=sell\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        # Observation space: OHLCV + RSI + Predicted_Price\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf,\n",
        "            shape=(self.window_size, 7),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def _get_prediction(self, window):\n",
        "        with torch.no_grad():\n",
        "            return self.predictor(torch.FloatTensor(window)).item()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * (1 + self.transaction_cost) # Consider transaction cost on buy\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance += current_price * (1 - self.transaction_cost) # Consider transaction cost on sell\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position  - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data))\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform(prediction.detach().numpy().reshape(1,-1))[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done, truncated, info  = env.step(action[0])\n",
        "    total_reward += reward\n",
        "print(f\"Total simulated profit with hybrid agent_)\n",
        "        window = self.data.iloc[\n",
        "            self.current_step - self.window_size:self.current_step\n",
        "        ][['Open', 'High', 'Low', 'Close', 'Volume', 'RSI']].values\n",
        "\n",
        "        # Get prediction for next day\n",
        "        prediction = self._get_prediction(\n",
        "            self.data['Close_scaled'].values[\n",
        "                self.current_step - self.window_size:self.current_step\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Add prediction to observation\n",
        "        return np.concatenate([window, np.full((self.window_size, 1), prediction)], axis=1)\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "        if action == 1 and self.position <= 0:  # Buy\n",
        "            self.position = 1\n",
        "            self.balance -= current_price * self.transaction_cost\n",
        "        elif action == 2 and self.position >= 0:  # Sell\n",
        "            self.position = -1\n",
        "            self.balance -= current_price * self.transaction_cost\n",
        "\n",
        "    def step(self, action):\n",
        "        self._take_action(action)\n",
        "        self.current_step += 1\n",
        "\n",
        "        prev_price = self.data.iloc[self.current_step - 1]['Close']\n",
        "        current_price = self.data.iloc[self.current_step]['Close']\n",
        "\n",
        "        # Risk-adjusted reward\n",
        "        price_change = current_price - prev_price\n",
        "        reward = price_change * self.position - abs(price_change) * self.transaction_cost\n",
        "\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        truncated = False\n",
        "\n",
        "        return self._next_observation(), reward, done, truncated, {}\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = self.window_size\n",
        "        self.position = 0\n",
        "        self.balance = 0\n",
        "        return self._next_observation(), {}\n",
        "\n",
        "# ======================\n",
        "# 4. Train RL Agent\n",
        "# ======================\n",
        "# Load predictor\n",
        "predictor.load_state_dict(torch.load(\"lstm_predictor.pth\"))\n",
        "predictor.eval()\n",
        "\n",
        "# Create environment\n",
        "env = HybridTradingEnv(data, predictor)\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Initialize and train PPO agent\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            policy_kwargs=dict(net_arch=dict(pi=[256, 256], vf=[256, 256])))\n",
        "model.learn(total_timesteps=50_000)\n",
        "\n",
        "# Save model\n",
        "model.save(\"hybrid_trading_agent\")\n",
        "\n",
        "# ======================\n",
        "# 5. Test & Predict\n",
        "# ======================\n",
        "def predict_next_day():\n",
        "    # Get latest data\n",
        "    latest_data = data['Close_scaled'].values[-sequence_length:]\n",
        "\n",
        "    # Predict next day\n",
        "    with torch.no_grad():\n",
        "        prediction = predictor(torch.FloatTensor(latest_data).item()\n",
        "\n",
        "    # Inverse transform\n",
        "    return close_scaler.inverse_transform([[prediction]])[0][0]\n",
        "\n",
        "# Get predictions\n",
        "next_day_price = predict_next_day()\n",
        "print(f\"Predicted next day closing price: ${next_day_price:.2f}\")\n",
        "\n",
        "# Test agent\n",
        "obs = env.reset()\n",
        "done = [False]\n",
        "total_reward = 0\n",
        "\n",
        "while not done[0]:\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, rewards, done, _ = env.step(action)\n",
        "    total_reward += rewards[0]\n",
        "\n",
        "print(f\"Total simulated profit with hybrid agent: ${total_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "QGokYdT1f6rL",
        "outputId": "02254cc3-2ba3-4d16-9273-db3824fb8a75"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 1138) (<ipython-input-26-89701fc75336>, line 1138)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-89701fc75336>\"\u001b[0;36m, line \u001b[0;32m1138\u001b[0m\n\u001b[0;31m    print(f\"Total simulated profit with hybrid agent: ${total_\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1138)\n"
          ]
        }
      ]
    }
  ]
}